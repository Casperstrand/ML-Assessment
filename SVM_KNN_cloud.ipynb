{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1668869376188
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Casper\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Casper\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Casper\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\Casper\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import model_selection, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemmetizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    text = [w for w in text if w.lower() not in stopwords]\n",
        "    return text\n",
        "\n",
        "def lemmetize_words(word_list):\n",
        "    lemmetized = [lemmetizer.lemmatize(w) for w in word_list]\n",
        "    return lemmetized\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    text = [w for w in text if w.isalpha()]\n",
        "    return text\n",
        "\n",
        "def remove_non_english_words(text):\n",
        "    printable = set(string.printable)\n",
        "    return [word for word in text \n",
        "            if all(char in printable for char in word)]\n",
        "\n",
        "def fix_text(row):\n",
        "    return ' '.join(row)\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "df = df.drop('author', axis=1)\n",
        "df = df.dropna()\n",
        "df['final_text'] = df['text'].apply(nltk.word_tokenize)\n",
        "df['final_text'] = df['final_text'].apply(remove_special_characters)\n",
        "df['final_text'] = df['final_text'].apply(remove_non_english_words)\n",
        "df['final_text'] = df['final_text'].apply(remove_stop_words)\n",
        "df['final_text'] = df['final_text'].apply(lemmetize_words)\n",
        "df['final_text'] = df['final_text'].apply(fix_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668804637735
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "vec_single = CountVectorizer()\n",
        "vec_single.fit(df['final_text'])\n",
        "bag_of_words = vec_single.transform(df['final_text'])\n",
        "sum_words = bag_of_words.sum(axis=0) \n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vec_single.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "\n",
        "x_single = []\n",
        "count_single = []\n",
        "\n",
        "for word, freq in words_freq[:20]:\n",
        "    x_single.append(word)\n",
        "    count_single.append(freq)\n",
        "\n",
        "plt.barh(x_single, count_single)\n",
        "plt.title(\"Top 20 words used in news article\")\n",
        "plt.xlabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668804686951
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "vec_double = CountVectorizer(ngram_range=(2,2))\n",
        "vec_double.fit(df['final_text'])\n",
        "bag_of_words = vec_double.transform(df['final_text'])\n",
        "sum_words = bag_of_words.sum(axis=0) \n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vec_double.vocabulary_.items()]\n",
        "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "\n",
        "x_double = []\n",
        "count_double = []\n",
        "\n",
        "for word, freq in words_freq[:20]:\n",
        "    x_double.append(word)\n",
        "    count_double.append(freq)\n",
        "\n",
        "plt.barh(x_double, count_double)\n",
        "plt.title('Top 20 bigrams used in news articles')\n",
        "plt.xlabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668804755456
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "vec_triple = CountVectorizer(ngram_range=(3,3))\n",
        "vec_triple.fit(df['final_text'])\n",
        "bag_of_words = vec_triple.transform(df['final_text'])\n",
        "sum_words = bag_of_words.sum(axis=0) \n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vec_triple.vocabulary_.items()]\n",
        "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "\n",
        "x_triple = []\n",
        "count_triple = []\n",
        "\n",
        "for word, freq in words_freq[:20]:\n",
        "    x_triple.append(word)\n",
        "    count_triple.append(freq)\n",
        "\n",
        "plt.barh(x_triple, count_triple)\n",
        "plt.title('Top 20 trigrams used in news articles')\n",
        "plt.xlabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1668869390834
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(df['final_text'], df['label'], test_size=0.2)\n",
        "\n",
        "Tfidf_vect = TfidfVectorizer()\n",
        "Tfidf_vect.fit(df['final_text'])\n",
        "Train_X_Tfidf = Tfidf_vect.transform(train_x)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1668868395082
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'C':[1, 10, 50, 100]}\n",
        "SVM_model_2 = svm.SVC()\n",
        "svm_grid = GridSearchCV(SVM_model_2, parameters)\n",
        "svm_grid.fit(Train_X_Tfidf, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668615230226
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(f'The best parameters for this model are: {svm_grid.best_params_} and score is {svm_grid.best_score_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668622060400
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "SVM_model = svm.SVC()\n",
        "SVM_model.fit(Train_X_Tfidf,train_y)\n",
        "predictions_SVM = SVM_model.predict(Test_X_Tfidf)\n",
        "print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, test_y)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668526074084
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "cm_svm = confusion_matrix(predictions_SVM, test_y)\n",
        "sns.heatmap(cm_svm/np.sum(cm_svm), annot=True, fmt='0.2%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668815479710
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier()\n",
        "parameters = {'n_neighbors' : [1,2,3,4,5,6,7,8,9,10]}\n",
        "knn_grid = GridSearchCV(knn, parameters, scoring='accuracy')\n",
        "knn_grid.fit(Train_X_Tfidf, train_y)\n",
        "knn_grid_pred = knn_grid.predict(Test_X_Tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668815486066
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(f'The best parameters for this model are: {knn_grid.best_params_} and score is {knn_grid.best_score_}')\n",
        "plt.title('Scores of Different Amount of Neighbours')\n",
        "plt.ylabel('Accuracy Score')\n",
        "plt.xlabel('Amount of Neigbours')\n",
        "plt.plot(knn_grid.cv_results_['mean_test_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668815535800
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "cm_knn = confusion_matrix(knn_grid_pred, test_y)\n",
        "plt.title('KNN Heatmap')\n",
        "sns.heatmap(cm_knn/np.sum(cm_knn), annot=True, fmt='0.2%')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668616411244
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "page = requests.get(\"https://www.bbc.com/news\")\n",
        "\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "df = pd.DataFrame(columns=['Text'])\n",
        "def find_text(link):\n",
        "    p = requests.get(link)\n",
        "    s = BeautifulSoup(p.content, \"html.parser\")\n",
        "    text = s.find_all(class_=\"ssrcss-1q0x1qg-Paragraph eq5iqo00\")\n",
        "    # author = s.find(class_ = \"ssrcss-68pt20-Text-TextContributorName e8mq1e96\")\n",
        "    list_of_text = []\n",
        "    for t in text:\n",
        "        list_of_text.append(t.text)\n",
        "    return ' '.join(list_of_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668624105570
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def article_pred(link):\n",
        "    text = find_text(link)\n",
        "    text = nltk.word_tokenize(text)\n",
        "    text = remove_special_characters(text)\n",
        "    text = remove_stop_words(text)\n",
        "    text = lemmetize_words(text)\n",
        "    text = fix_text(text)\n",
        "    text = Tfidf_vect.transform([text])\n",
        "    test_pred = SVM_model.predict(text)\n",
        "    return test_pred\n",
        "\n",
        "article_pred('https://www.bbc.com/news/world-us-canada-63463738')"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8930fd93c0d0e39a0d4d9dd43e222c1e36af1b68c0e251fae90799f6ab1738d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
